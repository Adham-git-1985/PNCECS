import requests
from bs4 import BeautifulSoup
import json
import time

base_url = "https://www.pncecs.plo.ps"
category_id = 3  # Ø±Ù‚Ù… Ø§Ù„ØªØµÙ†ÙŠÙ
page_number = 1
id_counter = 1

all_news = []
visited_links = set()

def extract_news_links(page_url):
    print(f"ğŸ“„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø©: {page_url}")
    resp = requests.get(page_url)
    resp.encoding = 'utf-8'
    if resp.status_code != 200:
        print("âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©.")
        return []

    soup = BeautifulSoup(resp.text, 'html.parser')
    links = []

    for article in soup.find_all("h2", class_="entry-title"):
        a_tag = article.find("a")
        if a_tag and a_tag["href"] not in visited_links:
            title = a_tag.text.strip()
            link = a_tag["href"]
            links.append({"title": title, "link": link})
            visited_links.add(link)

    return links

def extract_full_content(news_item):
    try:
        response = requests.get(news_item['link'])
        response.encoding = 'utf-8'
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            content_div = soup.find('div', class_='entry-content')
            if content_div:
                content_text = content_div.get_text(separator='\n', strip=True)
                news_item['content'] = content_text
            else:
                news_item['content'] = "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰."
        else:
            news_item['content'] = "âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ ØµÙØ­Ø© Ø§Ù„ØªÙØ§ØµÙŠÙ„."
    except Exception as e:
        news_item['content'] = f"âš ï¸ Ø®Ø·Ø£: {str(e)}"
    return news_item

# Ø§Ù„ØªØµÙØ­ Ø¹Ø¨Ø± Ø§Ù„ØµÙØ­Ø§Øª
while True:
    page_url = f"{base_url}/?cat={category_id}&paged={page_number}"
    news_links = extract_news_links(page_url)

    if not news_links:
        print("ğŸ›‘ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø®Ø¨Ø§Ø± Ø¬Ø¯ÙŠØ¯Ø©. Ø§Ù„ØªÙˆÙ‚Ù.")
        break

    print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(news_links)} Ø®Ø¨Ø± ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page_number}")
    for item in news_links:
        item['id'] = id_counter
        id_counter += 1
        full_item = extract_full_content(item)
        all_news.append(full_item)
        print(f"ğŸ“° ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬: {item['title'][:50]}...")
        time.sleep(1)

    page_number += 1

# Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
with open("full_news.json", "w", encoding="utf-8") as f:
    json.dump(all_news, f, ensure_ascii=False, indent=4)

print(f"\nâœ…âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(all_news)} Ø®Ø¨Ø±Ù‹Ø§ Ù…Ù† ÙƒÙ„ Ø§Ù„ØµÙØ­Ø§Øª ÙˆØ­ÙØ¸Ù‡Ø§ ÙÙŠ full_news.json")
