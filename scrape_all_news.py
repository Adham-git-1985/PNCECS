import requests
from bs4 import BeautifulSoup
import json
import time

base_url = "https://www.pncecs.plo.ps"
news_archive_url = f"{base_url}/?cat=3"

all_news = []
visited_links = set()

def extract_news_links(page_url):
    print(f"ğŸ“„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø©: {page_url}")
    resp = requests.get(page_url)
    resp.encoding = 'utf-8'
    if resp.status_code != 200:
        print("âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©.")
        return [], None

    soup = BeautifulSoup(resp.text, 'html.parser')
    links = []

    for article in soup.find_all("h2", class_="entry-title"):
        a_tag = article.find("a")
        if a_tag and a_tag["href"] not in visited_links:
            title = a_tag.text.strip()
            link = a_tag["href"]
            links.append({"title": title, "link": link})
            visited_links.add(link)

    return links, soup

def extract_full_content(news_item):
    try:
        response = requests.get(news_item['link'])
        response.encoding = 'utf-8'
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            content_div = soup.find('div', class_='entry-content')
            if content_div:
                content_text = content_div.get_text(separator='\n', strip=True)
                news_item['content'] = content_text
            else:
                news_item['content'] = "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰."
        else:
            news_item['content'] = "âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ ØµÙØ­Ø© Ø§Ù„ØªÙØ§ØµÙŠÙ„."
    except Exception as e:
        news_item['content'] = f"âš ï¸ Ø®Ø·Ø£: {str(e)}"
    return news_item


# Ù†Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ¹Ø±Ø§Ø¶ Ø­ØªÙ‰ 30 ØµÙØ­Ø© ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰
for page in range(1, 30):
    url = f"{news_archive_url}&paged={page}"
    news_links, soup = extract_news_links(url)

    if not news_links:
        print(f"â›” Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø®Ø¨Ø§Ø± ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page}ØŒ Ø§Ù„ØªÙˆÙ‚Ù.")
        break

    print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(news_links)} Ø®Ø¨Ø± ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page}")
    for item in news_links:
        full_item = extract_full_content(item)
        all_news.append(full_item)
        print(f"ğŸ“° ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬: {item['title'][:50]}...")
        time.sleep(1)

# Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
with open("full_news.json", "w", encoding="utf-8") as f:
    json.dump(all_news, f, ensure_ascii=False, indent=4)

print(f"\nâœ…âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(all_news)} Ø®Ø¨Ø±Ù‹Ø§ Ù…Ù† Ø¹Ø¯Ø© ØµÙØ­Ø§Øª ÙˆØ­ÙØ¸Ù‡Ø§ ÙÙŠ full_news.json")
