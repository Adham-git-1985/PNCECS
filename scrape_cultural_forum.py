import os
import re
import json
import requests
from bs4 import BeautifulSoup
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ---
urls = [
    "https://www.pncecs.plo.ps/?page_id=5867",
    "https://www.pncecs.plo.ps/?p=5841",
    "https://www.pncecs.plo.ps/?p=5785",
    "https://www.pncecs.plo.ps/?p=5827",
    "https://www.pncecs.plo.ps/?p=5832",
    "https://www.pncecs.plo.ps/?p=5822",
    "https://www.pncecs.plo.ps/?p=5811",
    "https://www.pncecs.plo.ps/?p=5802",
    "https://www.pncecs.plo.ps/?p=5791",
    "https://www.pncecs.plo.ps/?p=5853",
    "https://www.pncecs.plo.ps/?p=5777",
    "https://www.pncecs.plo.ps/?p=5769",
    "https://www.pncecs.plo.ps/?p=5761",
    "https://www.pncecs.plo.ps/?p=5752"
]

# --- Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ---
image_dir = os.path.join("static", "images", "cultural_forum")
os.makedirs(image_dir, exist_ok=True)

json_path = "cultural_forum_data.json"
existing_data = []

if os.path.exists(json_path):
    with open(json_path, "r", encoding="utf-8") as f:
        existing_data = json.load(f)

titles_in_json = [x["title"] for x in existing_data]

# --- Ø¯Ø§Ù„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ® ---
def extract_date(text):
    """
    ØªØ¨Ø­Ø« Ø¹Ù† Ø£Ù†Ù…Ø§Ø· ØªÙˆØ§Ø±ÙŠØ® Ø¹Ø±Ø¨ÙŠØ© Ù…Ø«Ù„ 27/08/2018 Ø£Ùˆ 3/9/2018
    """
    match = re.search(r"(\d{1,2})[/-](\d{1,2})[/-](\d{4})", text)
    if match:
        d, m, y = match.groups()
        return f"{int(y):04d}-{int(m):02d}-{int(d):02d}"
    return None


# --- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆÙ‰ ØµÙØ­Ø© ÙˆØ§Ø­Ø¯Ø© ---
def scrape_page(url):
    try:
        print(f"ğŸ“° Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø©: {url}")
        res = requests.get(url, verify=False, timeout=20)
        if res.status_code != 200:
            print(f"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø©: {url}")
            return None

        soup = BeautifulSoup(res.text, "html.parser")

        # --- Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ---
        title_tag = soup.find("h1", class_="entry-title")
        title = title_tag.get_text(strip=True) if title_tag else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"

        # --- Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ---
        content_div = soup.find("div", class_="entry-content")
        content = str(content_div) if content_div else ""

        # --- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ® Ù…Ù† Ø§Ù„Ù†Øµ ---
        text_for_date = content_div.get_text(" ", strip=True) if content_div else ""
        date = extract_date(text_for_date)

        # --- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØ± ---
        image_filenames = []
        if content_div:
            img_tags = content_div.find_all("img")
            for img_tag in img_tags:
                img_url = img_tag.get("src")
                if not img_url:
                    continue

                filename = os.path.basename(img_url.split("?")[0])
                image_path = os.path.join(image_dir, filename)

                # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
                if not os.path.exists(image_path):
                    try:
                        img_data = requests.get(img_url, verify=False, timeout=10).content
                        with open(image_path, "wb") as f:
                            f.write(img_data)
                        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©: {filename}")
                    except Exception as e:
                        print(f"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© {img_url}: {e}")
                        continue
                else:
                    print(f"â­ï¸ Ø§Ù„ØµÙˆØ±Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§: {filename}")

                if filename not in image_filenames:
                    image_filenames.append(filename)

        return {
            "title": title,
            "content": content,
            "date": date or "",
            "images": image_filenames
        }

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© {url}: {e}")
        return None


# --- Ø§Ù„ØªÙ†ÙÙŠØ° ---
new_data = []
for link in urls:
    result = scrape_page(link)
    if result:
        # Ø¥Ø°Ø§ Ø§Ù„Ø®Ø¨Ø± Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ù‹Ø§ â†’ Ø­Ø¯Ø«Ù‡ Ø¨Ø¯Ù„ Ø¥Ø¶Ø§ÙØªÙ‡
        updated = False
        for item in existing_data:
            if item["title"] == result["title"]:
                item.update(result)
                updated = True
                break
        if not updated:
            new_data.append(result)

# Ø¯Ù…Ø¬ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù…Ø¹ Ø§Ù„Ù‚Ø¯ÙŠÙ…
if new_data:
    existing_data.extend(new_data)

# Ø­ÙØ¸ Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª
with open(json_path, "w", encoding="utf-8") as f:
    json.dump(existing_data, f, ensure_ascii=False, indent=2)

print(f"\nâœ… ØªÙ… ØªØ­Ø¯ÙŠØ« {len(existing_data)} Ø®Ø¨Ø±Ù‹Ø§ ÙÙŠ {json_path}")
print(f"ğŸ“ Ø§Ù„ØµÙˆØ± Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ: {image_dir}")
